{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymongo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9807db06d7ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msplinter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBrowser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpymongo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#set path to use splinter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mexecutable_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'executable_path'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'chromedriver.exe'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymongo'"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import codecs\n",
    "import pymongo\n",
    "#set path to use splinter\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "#declare url and parser\n",
    "url = \"https://en.wikipedia.org/wiki/Lists_of_shipwrecks\"\n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "soup = bs(html,'html.parser')\n",
    "\n",
    "url = []\n",
    "#find desired data\n",
    "for i in soup.find('ul').findAll('a'):\n",
    "    url.append(i['href'])\n",
    "\n",
    "\n",
    "    #click link to go to country\n",
    "   # browser.click_link_by_partial_href(i['href'])\n",
    "   # browser.back()\n",
    "\n",
    "    #obtain new page html and parse\n",
    "    #html_country = browser.html\n",
    "    #soup_country = bs(html_country,\"html.parser\")\n",
    "\n",
    "    #find all tables\n",
    "    #wikitables = soup.findAll(\"table\", class_ ='wikitable').findall()\n",
    "    #pd.read_html(\"https://en.wikipedia.org\" + i[\"href\"])\n",
    "    #if i > 1:\n",
    "    #     break\n",
    "tables = []\n",
    "for i in url:\n",
    "    tables.append(pd.read_html(\"https://en.wikipedia.org\" + i))\n",
    "\n",
    "#shipwreck_df = pd.DataFrame(tables)\n",
    "\n",
    "# df = tables[0][0]\n",
    "# df.columns = ['ship','sunk_date','notes','coordinates']\n",
    "# print(df.head())\n",
    "\n",
    "df = tables[0][2]\n",
    "#df.columns = ['ship','sunk_date','notes','coordinates']\n",
    "#print(df.head())\n",
    "#df = df.reindex(df.index.drop(0)).reset_index(drop=True)\n",
    "df\n",
    "print(df)\n",
    "\n",
    "unpacklist = [x for table in tables for x in table]\n",
    "new_unpacklist = unpacklist.remove(unpacklist[0])\n",
    "new_df = pd.concat(unpacklist)\n",
    "#new_df = pd.DataFrame(unpacklist)\n",
    "#new_df.columns = ['ship','sunk_date','notes','coordinates']\n",
    "#new_df= df.reindex(df.index.drop(0)).reset_index(drop=True)\n",
    "\n",
    "ship_wreck_data = new_df.drop(columns = [4,5,6,7,8])\n",
    "ship_wreck_data = ship_wreck_data.dropna()\n",
    "ship_wreck_data.columns= ['ship','sunk_date','notes','coordinates']\n",
    "#ship_wreck_data = df.reindex(df.index.drop(0)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "ship_wreck_data = ship_wreck_data.drop_duplicates()\n",
    "ship_wreck_data = ship_wreck_data.drop([0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_wreck_data = ship_wreck_data.reset_index(drop=True)\n",
    "#test = ship_wreck_data['coordinates'].str.encode(\"utf-8\")\n",
    "#test  = test.str.decode(\"utf-8\")\n",
    "#test \n",
    "\n",
    "#ship_wreck_data['coordinates'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_coordinates = []\n",
    "#ship_wreck_data1 = ship_wreck_data.drop([0,74])\n",
    "\n",
    "for coordinate in ship_wreck_data['coordinates']:\n",
    "    if coordinate.find('/')==-1:\n",
    "        print(coordinate)\n",
    "    else:\n",
    "        latlong = coordinate.split('/')[1]\n",
    "        new_latlong = latlong.replace(\"°\",\"\").strip()\n",
    "        lat = new_latlong.split()[0]\n",
    "        long = new_latlong.split()[1]\n",
    "        lat = lat[1:]\n",
    "    \n",
    "        #print(lat[:len(lat)-2])\n",
    "        if lat[len(lat)-1] == 'S':\n",
    "            lat = 0-float(lat[:len(lat)-2])\n",
    "           # print(lat)\n",
    "        else:\n",
    "            lat = lat.strip(\"N\")\n",
    "            #print(lat)\n",
    "        if long[len(long)-1] == 'W':\n",
    "            long = 0-float(long[:len(long)-2])\n",
    "            #print(long)\n",
    "        else:\n",
    "            long = long.strip(\"E\")\n",
    "        #print(long)\n",
    "    clean_coordinates = [lat,long]\n",
    "    print(clean_coordinates)\n",
    "    true_coordinates.append(clean_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert coordinates into dataframe to clean data\n",
    "true_coordinates_df = pd.DataFrame(true_coordinates)\n",
    "true_coordinates_df.columns = ['Latitude', 'Longitude']\n",
    "\n",
    "#drop duplicate values\n",
    "true_coordinates_df =true_coordinates_df.drop_duplicates()\n",
    "\n",
    "#convert true_coordinates_df to a list then into a series so both values will be in one column as coordinates\n",
    "coordinate = true_coordinates_df.values.tolist()\n",
    "coordinate_series = pd.Series(coordinate)\n",
    "\n",
    "#append coordinate_series to coordinate column and drop Nan values\n",
    "ship_wreck_df = ship_wreck_data\n",
    "ship_wreck_df['coordinates'] = coordinate_series\n",
    "ship_wreck_df =ship_wreck_df.dropna()\n",
    "ship_wreck_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to json\n",
    "import json\n",
    "ship_wreck_json = ship_wreck_df.to_json(orient='records')\n",
    "json_format = json.loads(ship_wreck_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "#create mongo database\n",
    "myclient = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "db = myclient['ship_wreck_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = db.shipwreckData\n",
    "json_data.drop()\n",
    "json_data.insert_many(json_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_wreck_df.loc[ship_wreck_df['ship'] == 'HMS Matabele']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
